{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis of Youtube product comments (Xiaomi brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - VADER for sentiment analysis\n",
    "\n",
    "- The extracted youtube comments saved as all_comment.text format. You can find it in the data_folder. \n",
    "- In this part, I 'm creating a dataset to apply Vader on it\n",
    "- The reason for this sepratation of dataset for TextBlob and VADER is that, for VADER, it's not necessary to change the uppercase to lowercase, becasue it will count it to measure polarity and subjectivity. \n",
    "- Or it consider emojies to analysis the sentiment of text.\n",
    "- It works well on the text from social media. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, os, sys, math, random\n",
    "\n",
    "######\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "nltk.download('words')\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "nltk.download('words')\n",
    "\n",
    "###### Prevent future/deprecation warnings from showing in output\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Consideration-VADER: \n",
    "- No need to remove stop words separately. \n",
    "- No need to remove emoji --> :)\n",
    "- No need to change upper case to lower case -->\n",
    "- No need to remove exclamation mark --> \"!\"\n",
    "- No need to remove acronyms -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus_path_v = open(\"/home/unina/PycharmProjects/Com_txt/Com_txt/data_folder/all_comment.text\", \"r\", encoding = \"utf-8-sig\").read()\n",
    "print(my_corpus_path_v[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######1. Remove http link ######\n",
    "my_v_F = re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?\\S\", \"\", my_corpus_path_v)\n",
    "my_v_FF = re.sub(\"[0-9]\", \"\", my_v_F)\n",
    "print(my_v_FF[:100])\n",
    "my_corpus_v_2 = my_v_FF.strip()\n",
    "\n",
    "#print(len(my_corpus_v_2))\n",
    "print(my_corpus_v_2[:100])\n",
    "corpus_word_tokens_v = word_tokenize(my_corpus_v_2)\n",
    "#print(corpus_word_tokens_v[:100])\n",
    "\n",
    "######4. Remove #######\n",
    "list_abb = [\"/\", \"$\", \" \", \"-\", \"(\", \")\"]\n",
    "my_corpus_list_v = [\" \".join([w for w in t.split() if not w in list_abb]) for t in corpus_word_tokens_v]\n",
    "print(my_corpus_list_v[:100])\n",
    "print(\"END1\"+\"\\n\")\n",
    "######5. Join ######\n",
    "my_corpus_join_v=  ' '.join(my_corpus_list_v)\n",
    "print(my_corpus_join_v[7000:8000]+\"\\n\"+\"END2\")\n",
    "print(len(my_corpus_join_v))\n",
    "\n",
    "######6. Remove non English words #######\n",
    "\n",
    "non_english_v = set(nltk.corpus.words.words())\n",
    "\n",
    "my_corpus_clean_v = \" \".join(w for w in nltk.wordpunct_tokenize(my_corpus_join_v) \\\n",
    "         if w.lower() in non_english_v or not w.isalpha())\n",
    "print(\"\\n\"+my_corpus_clean_v[8000:10500]+\"\\n\"+\"END3\")\n",
    "print(len(my_corpus_clean_v))\n",
    "print(type(my_corpus_clean_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/unina/PycharmProjects/Com_txt/Com_txt/data_folder/my_corpus_vader.txt\", \"w\", encoding = \"utf-8-sig\") as f:\n",
    "    f.write(my_corpus_clean_v)\n",
    "    \n",
    "##Next--> check two copora with TextBlob & VADER.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
