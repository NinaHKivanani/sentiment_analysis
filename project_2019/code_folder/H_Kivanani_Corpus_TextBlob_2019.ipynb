{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sentiment analysis of Youtube product comments (Xiaomi brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TextBlob for sentiment analysis\n",
    "- I checked the extracted dataset with textblob and the next step, I will check the corpus with Vader, and compare both results together to see which one performs better. \n",
    "- The dataset is consisted of a list of phone reviews on Youtube.\n",
    "- For using TextBlob, I need to clean data. \n",
    "- You can find dataset (youtube comments) in data folder with the name **all_comment.text**.\n",
    "\n",
    "Note : you can extract your own data by changing http in the `H_Kivanani_Preparation_com_txt_2019.ipynb`, so it's better to work with your own dataset and compare mine with yours!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')# Prevent future/deprecation warnings from showing in output\n",
    "\n",
    "#####Text##### \n",
    "import nltk, re, string, requests, os, sys, math, random\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "#There are a number of tokenizator that can be used to change plain text to numeric one: spaCy, keras, NLTK.\n",
    "#For this study, I will use NLTK to tokenize our data. \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "############\n",
    "import demoji\n",
    "demoji.download_codes() # Run this evrytime if you want to get updated emoji\n",
    "#from collections import defaultdict\n",
    "#from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####0. Open data #######\n",
    "my_path = open(\"/home/unina/PycharmProjects/Com_txt/Com_txt/data_folder/all_comment.text\", \"r\", encoding = \"utf-8-sig\").read()\n",
    "print(my_path[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  - Prepare the corpus-TextBlob\n",
    "Note: From the above output, it seems I need to clean data from some noise! Such as URL, Stopwords.\n",
    "\n",
    "The steps that I used are as follow:\n",
    "1. remove punctuations, numbers \n",
    "2. remove stopwords (a, about, so, all, the, ...)\n",
    "3. use lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######1. Remove http link ######\n",
    "my_corpus_path_n = re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?\\S\", \"\", my_path)\n",
    "my_corpus_path_n = re.sub(\"[0-9]\", \"\", my_corpus_path_n)\n",
    "\n",
    "######Lowercase-------------------------------------------\n",
    "my_corpus_path_n= my_corpus_path_n.lower()\n",
    "my_corpus = my_corpus_path_n.strip()\n",
    "print(my_corpus[1:100]+\" \"+\"HERE\")\n",
    "print(len(my_corpus))\n",
    "\n",
    "######2. Remove Emoji ######\n",
    "\n",
    "#For removing Emoji, I used reg, but It couldn't find some emjoi such as \"fire\". So I switch to \"deemoji\".\n",
    "#If you want to see the result, just uncomment the emoji_pattern code.\n",
    "# emoji_pattern = re.compile(\"[\"\n",
    "#         u\"\\U0001F600-\\U0001F64F\"  \n",
    "#         u\"\\U0001F300-\\U0001F5FF\"  \n",
    "#         u\"\\U0001F680-\\U0001F6FF\"  \n",
    "#         u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "#         u\"\\U0001F1F2-\\U0001F1F4\" \n",
    "#         u\"\\U0001F1E6-\\U0001F1FF\"\n",
    "#         u\"\\U0001F600-\\U0001F64F\"\n",
    "#         u\"\\U00002702-\\U000027B0\"\n",
    "#         u\"\\U000024C2-\\U0001F251\"\n",
    "#         u\"\\U0001f926-\\U0001f937\"\n",
    "#         u\"\\U0001F1F2\"\n",
    "#         u\"\\U0001F1F4\"\n",
    "#         u\"\\U0001F620\"\n",
    "#         u\"\\u200d\"\n",
    "#         u\"\\u2640-\\u2642\"\n",
    "#         u\"\\u2600-\\u2B55\"\n",
    "#         u\"\\u23cf\"\n",
    "#         u\"\\u23e9\"\n",
    "#         u\"\\u231a\"\n",
    "#         u\"\\u3030\"\n",
    "#         u\"\\ufe0f\"\n",
    "#         \"ðŸ¤”ðŸ¤”ðŸ¤—ðŸ¤‘ðŸ¤‘^_^ðŸ‡®ðŸ‡³ðŸ”¥\"\n",
    "#         \"]+\", flags=re.UNICODE)\n",
    "# my_corpus_emoji = emoji_pattern.sub(r'', my_corpus)\n",
    "\n",
    "print(\"\\n\")\n",
    "my_corpus_emoji= demoji.replace(my_corpus)\n",
    "print(my_corpus_emoji[:200])\n",
    "\n",
    "######3. Remove stopwords ######\n",
    "stop_words = list(get_stop_words('en'))\n",
    "nltk_words = list(stopwords.words('english'))   \n",
    "nltk_words.extend(stop_words)\n",
    "corpus_word_tokens = word_tokenize(my_corpus_emoji)\n",
    "corpus_stop_words = [w for w in corpus_word_tokens if not w in stop_words]\n",
    "#corpus_stop_words = []\n",
    "#for w in corpus_word_tokens:\n",
    "    #if w not in stop_words:\n",
    "        #corpus_stop_words.append(w)\n",
    "corpus_single_len = [word for word in corpus_stop_words if len(word) > 1]  \n",
    "\n",
    "######4. Remove abbriviations #######\n",
    "list_abb = [\"'m\", \"'s\",\"'re\",\"'ll\", \"n't\",\".\", \"?\", \"!\", \"im\", \"/\", \"...\", \"''\", '``', '<', \"@\", \":\", \"u\",\n",
    "            \"Â¡Â¿Â¡Â¿\",\"$\", \"e\", \"s\",\" + \", \"'e'\", \"'s'\"]\n",
    "my_corpus_list = [\" \".join([w for w in t.split() if not w in list_abb]) for t in corpus_single_len]\n",
    "#print(my_corpus_list[:100])\n",
    "\n",
    "\n",
    "######5. Join ######\n",
    "my_corpus_join=  ' '.join(my_corpus_list)\n",
    "print(\"\\n\"+my_corpus_join[10200:10800]+\"\\n\")\n",
    "print(len(my_corpus_join))\n",
    "\n",
    "######6. Remove non English words #######\n",
    "\n",
    "non_english = set(nltk.corpus.words.words())\n",
    "\n",
    "my_corpus_clean = \" \".join(w for w in nltk.wordpunct_tokenize(my_corpus_join) \\\n",
    "         if w.lower() in non_english or not w.isalpha())\n",
    "print(\"\\n\"+my_corpus_clean[8000:8500])\n",
    "print(len(my_corpus_clean))\n",
    "print(type(my_corpus_clean))\n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - List of emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emoji = demoji.findall(my_corpus)\n",
    "print(test_emoji)#If you want to see which kind of emojis you have in your corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save your clean data for TextBlob part ---> my_corpus_textblob.txt\n",
    "with open(\"/home/unina/PycharmProjects/Com_txt/Com_txt/data_folder/my_corpus_textblob.txt\", \"w\", encoding = \"utf-8-sig\") as f:\n",
    "    f.write(my_corpus_clean)\n",
    "\n",
    "######Next --> corpus for VADER    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "As the lenght of corpus showed after preprocessing, we lost a lot of data for TextBlob."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
