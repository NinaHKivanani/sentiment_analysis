{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sentiment analysis of Youtube product comments (Xiaomi brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TextBlob for sentiment analysis\n",
    "- I checked the extracted dataset with textblob and the next step, I will check the corpus with Vader, and compare both results together to see which one performs better. \n",
    "- The dataset is consisted of a list of phone reviews on Youtube.\n",
    "- For using TextBlob, I need to clean data. \n",
    "- You can find dataset (youtube comments) in data folder with the name **all_comment.text**.\n",
    "\n",
    "Note : you can extract your own data by changing http in the `H_Kivanani_Preparation_com_txt_2019.ipynb`, so it's better to work with your own dataset and compare mine with yours!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/unina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/unina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/unina/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 1.24 seconds)\n",
      "\u001b[33mWriting emoji data to /home/unina/.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')# Prevent future/deprecation warnings from showing in output\n",
    "\n",
    "#####Text##### \n",
    "import nltk, re, string, requests, os, sys, math, random\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "#There are a number of tokenizator that can be used to change plain text to numeric one: spaCy, keras, NLTK.\n",
    "#For this study, I will use NLTK to tokenize our data. \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "############\n",
    "import demoji\n",
    "demoji.download_codes() # Run this evrytime if you want to get updated emoji\n",
    "#from collections import defaultdict\n",
    "#from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥Xiaomi Mi A2 is only $209 right now (limited) - http://click.aliexpress.com/e/cXMSfnnv 🔥\n",
      "When xiaom\n"
     ]
    }
   ],
   "source": [
    "#####0. Open data #######\n",
    "my_path = open(\"/home/unina/PycharmProjects/Com_txt/Com_txt/data_folder/all_comment.text\", \"r\", encoding = \"utf-8-sig\").read()\n",
    "print(my_path[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  - Prepare the corpus-TextBlob\n",
    "Note: From the above output, it seems I need to clean data from some noise! Such as URL, Stopwords.\n",
    "\n",
    "The steps that I used are as follow:\n",
    "1. remove punctuations, numbers \n",
    "2. remove stopwords (a, about, so, all, the, ...)\n",
    "3. use lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xiaomi mi a is only $ right now (limited) -  🔥\n",
      "when xiaomi reaches $\n",
      "iphone cost a soul\n",
      "i think i w HERE\n",
      "81054\n",
      "\n",
      "\n",
      "xiaomi mi a is only $ right now (limited) -  \n",
      "when xiaomi reaches $\n",
      "iphone cost a soul\n",
      "i think i will be switching to android for my next phone. apple keeps getting more expensive\n",
      "i bought xiaomi mi a\n",
      "\n",
      "battery mom uses got mi courtesy xiaomi yet use experience using xiaomi phones past camera quality really good unbox xiaomi redmi note pro  xiaomi fan really xiaomi usually faster canada  uk based love channel cool see review nokia can get hold one india guy started becoming mi fanboy review mi lite light version something like half power one btw respect someone respect stock android ^_^  deciding lite huawei smart samsung motorola play re missing reviews consider phone redmi note pro many say nokia plus better big battery better camera  review nokia plus mobile best buy budget minimum ali exp\n",
      "\n",
      "52157\n",
      "\n",
      "actually mi pro new notch design favor uniformity correction .. + rear camera setup .. + sir s + mi best everything goes well going lite anyone else watching mi watching note pro please mi lite well mi lite note pro watching video mi prime dude juice got note global week ago now best buck stock android top everything good love phone rear camera get included camera depth may something else edit secondary used night combined primary camera rear camera actually plus miss mi ram mi ram choice good s\n",
      "38061\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/unina/PycharmProjects/Com_txt/Com_txt/code_folder'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######1. Remove http link ######\n",
    "my_corpus_path_n = re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?\\S\", \"\", my_path)\n",
    "my_corpus_path_n = re.sub(\"[0-9]\", \"\", my_corpus_path_n)\n",
    "\n",
    "######Lowercase-------------------------------------------\n",
    "my_corpus_path_n= my_corpus_path_n.lower()\n",
    "my_corpus = my_corpus_path_n.strip()\n",
    "print(my_corpus[1:100]+\" \"+\"HERE\")\n",
    "print(len(my_corpus))\n",
    "\n",
    "######2. Remove Emoji ######\n",
    "\n",
    "#For removing Emoji, I used reg, but It couldn't find some emjoi such as \"fire\". So I switch to \"deemoji\".\n",
    "#If you want to see the result, just uncomment the emoji_pattern code.\n",
    "# emoji_pattern = re.compile(\"[\"\n",
    "#         u\"\\U0001F600-\\U0001F64F\"  \n",
    "#         u\"\\U0001F300-\\U0001F5FF\"  \n",
    "#         u\"\\U0001F680-\\U0001F6FF\"  \n",
    "#         u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "#         u\"\\U0001F1F2-\\U0001F1F4\" \n",
    "#         u\"\\U0001F1E6-\\U0001F1FF\"\n",
    "#         u\"\\U0001F600-\\U0001F64F\"\n",
    "#         u\"\\U00002702-\\U000027B0\"\n",
    "#         u\"\\U000024C2-\\U0001F251\"\n",
    "#         u\"\\U0001f926-\\U0001f937\"\n",
    "#         u\"\\U0001F1F2\"\n",
    "#         u\"\\U0001F1F4\"\n",
    "#         u\"\\U0001F620\"\n",
    "#         u\"\\u200d\"\n",
    "#         u\"\\u2640-\\u2642\"\n",
    "#         u\"\\u2600-\\u2B55\"\n",
    "#         u\"\\u23cf\"\n",
    "#         u\"\\u23e9\"\n",
    "#         u\"\\u231a\"\n",
    "#         u\"\\u3030\"\n",
    "#         u\"\\ufe0f\"\n",
    "#         \"🤔🤔🤗🤑🤑^_^🇮🇳🔥\"\n",
    "#         \"]+\", flags=re.UNICODE)\n",
    "# my_corpus_emoji = emoji_pattern.sub(r'', my_corpus)\n",
    "\n",
    "print(\"\\n\")\n",
    "my_corpus_emoji= demoji.replace(my_corpus)\n",
    "print(my_corpus_emoji[:200])\n",
    "\n",
    "######3. Remove stopwords ######\n",
    "stop_words = list(get_stop_words('en'))\n",
    "nltk_words = list(stopwords.words('english'))   \n",
    "nltk_words.extend(stop_words)\n",
    "corpus_word_tokens = word_tokenize(my_corpus_emoji)\n",
    "corpus_stop_words = [w for w in corpus_word_tokens if not w in stop_words]\n",
    "#corpus_stop_words = []\n",
    "#for w in corpus_word_tokens:\n",
    "    #if w not in stop_words:\n",
    "        #corpus_stop_words.append(w)\n",
    "corpus_single_len = [word for word in corpus_stop_words if len(word) > 1]  \n",
    "\n",
    "######4. Remove abbriviations #######\n",
    "list_abb = [\"'m\", \"'s\",\"'re\",\"'ll\", \"n't\",\".\", \"?\", \"!\", \"im\", \"/\", \"...\", \"''\", '``', '<', \"@\", \":\", \"u\",\n",
    "            \"¡¿¡¿\",\"$\", \"e\", \"s\",\" + \", \"'e'\", \"'s'\"]\n",
    "my_corpus_list = [\" \".join([w for w in t.split() if not w in list_abb]) for t in corpus_single_len]\n",
    "#print(my_corpus_list[:100])\n",
    "\n",
    "\n",
    "######5. Join ######\n",
    "my_corpus_join=  ' '.join(my_corpus_list)\n",
    "print(\"\\n\"+my_corpus_join[10200:10800]+\"\\n\")\n",
    "print(len(my_corpus_join))\n",
    "\n",
    "######6. Remove non English words #######\n",
    "\n",
    "non_english = set(nltk.corpus.words.words())\n",
    "\n",
    "my_corpus_clean = \" \".join(w for w in nltk.wordpunct_tokenize(my_corpus_join) \\\n",
    "         if w.lower() in non_english or not w.isalpha())\n",
    "print(\"\\n\"+my_corpus_clean[8000:8500])\n",
    "print(len(my_corpus_clean))\n",
    "print(type(my_corpus_clean))\n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - List of emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'🙏': 'folded hands', '🔥': 'fire', '😊': 'smiling face with smiling eyes', '🇮🇳': 'flag: India', '👌🏼': 'OK hand: medium-light skin tone', '🤘': 'sign of the horns', '♥️': 'heart suit', '😉': 'winking face', '🤣': 'rolling on the floor laughing', '😄': 'grinning face with smiling eyes', '😰': 'anxious face with sweat', '✌️': 'victory hand', '😂': 'face with tears of joy', '😋': 'face savoring food', '😁': 'beaming face with smiling eyes', '😝': 'squinting face with tongue', '😕': 'confused face', '😜': 'winking face with tongue', '😗': 'kissing face', '😎': 'smiling face with sunglasses', '☺️': 'smiling face', '😀': 'grinning face', '🤔': 'thinking face', '🙏🏽': 'folded hands: medium skin tone', '👌': 'OK hand', '🤑': 'money-mouth face', '🔋': 'battery', '👍': 'thumbs up', '😭': 'loudly crying face', '😱': 'face screaming in fear', '🙌': 'raising hands', '😍': 'smiling face with heart-eyes', '😆': 'grinning squinting face', '😛': 'face with tongue', '💙': 'blue heart', '😇': 'smiling face with halo', '😈': 'smiling face with horns', '😘': 'face blowing a kiss', '🤗': 'hugging face', '😐': 'neutral face', '😃': 'grinning face with big eyes', '😏': 'smirking face', '❤️': 'red heart', '😑': 'expressionless face', '😢': 'crying face', '😅': 'grinning face with sweat', '🙏🏻': 'folded hands: light skin tone'}\n"
     ]
    }
   ],
   "source": [
    "test_emoji = demoji.findall(my_corpus)\n",
    "print(test_emoji)#If you want to see which kind of emojis you have in your corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save your clean data for TextBlob part ---> my_corpus_textblob.txt\n",
    "with open(\"/home/unina/PycharmProjects/Com_txt/Com_txt/data_folder/my_corpus_textblob.txt\", \"w\", encoding = \"utf-8-sig\") as f:\n",
    "    f.write(my_corpus_clean)\n",
    "\n",
    "######Next --> corpus for VADER    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "As the lenght of corpus showed after preprocessing, we lost a lot of data for TextBlob."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
